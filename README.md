# Laplacian Embedding via Iterative Eigendecomposition of a Regularized Inverse Laplacian

**&lt;ABSTRACT&gt;**
<div align="justify"> 
&nbsp;&nbsp;In the era of big data, data analysis tasks often involve dimension reduction to reduce the complexity of data and effectively visualize them. An answer to this everyday challenge in modern data analysis is Laplacian embedding, a non-linear dimension reduction technique (or a manifold learning method) that embed data onto Laplacian eigenmaps, a subspace spanned by eigenvectors corresponding to the least dominant eigenvalues of the graph Laplacian. Its bottleneck is, however, the eigendecomposition of the Laplacian matrix, which is characterized by the cubic time complexity. Furthermore, the conventional scheme is hardly efficient in that the majority of eigenpairs become redundant, after some of them are singled out to construct the Laplacian eigenmaps. To overcome these issues, we apply iterative methods to the inverse of a regularized Laplacian matrix and thereby selectively obtain only few eigenpairs necessary for constructing Laplacian eigenmaps. This approach not only lowers the time complexity of Laplacian embedding from cubic to quadratic, but also leads to improved representations according to our numerical experiments with toy datasets in Scikit Learn. Furthermore, its quantum translation is derived based on the block encoding technique.
</div>

